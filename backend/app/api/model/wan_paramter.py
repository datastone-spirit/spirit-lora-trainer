from dataclasses import dataclass

@dataclass
class WanVTrainingConfig:
    async_upload: bool = False
    base_weights: str  = None
    base_weights_multiplier: float = None
    blocks_to_swap: int = None
    clip: str  = None
    config_file: str  = None
    dataset_config = None
    ddp_gradient_as_bucket_view: bool = False
    ddp_static_graph: bool = False
    ddp_timeout: int = None
    dim_from_weights: bool = False
    discrete_flow_shift: float = 1.0
    dit: str  = None
    flash3: bool = False
    flash_attn: bool = False
    fp8_base: bool = False
    fp8_scaled: bool = False
    fp8_t5: bool = False
    gradient_accumulation_steps: int = 1
    gradient_checkpointing: bool = False
    guidance_scale: float = 1.0
    huggingface_path_in_repo: str  = None
    huggingface_repo_id: str  = None
    huggingface_repo_type: str  = None
    huggingface_repo_visibility: str  = None
    huggingface_token: str  = None
    img_in_txt_in_offloading: bool = False
    learning_rate: float = 2e-06
    log_config: bool = False
    log_prefix: str  = None # No need send from frontend
    log_tracker_config: str  = None
    log_tracker_name: str  = None
    log_with: str  = None
    logging_dir: str  = None # No need send from frontend
    logit_mean: float = 0.0
    logit_std: float = 1.0
    lr_decay_steps: int = 0
    lr_scheduler: str  = "constant"
    lr_scheduler_args: str  = None
    lr_scheduler_min_lr_ratio: float = None
    lr_scheduler_num_cycles: int = 1
    lr_scheduler_power: float = 1
    lr_scheduler_timescale: int = None
    lr_scheduler_type: str  = ""
    lr_warmup_steps: int = 0
    max_data_loader_n_workers: int = 8
    max_grad_norm: float = 1.0
    max_timestep: int = None
    max_train_epochs: int = None
    max_train_steps: int = 1600
    metadata_author: str  = None
    metadata_description: str  = None
    metadata_license: str  = None
    metadata_tags: str  = None
    metadata_title: str  = None
    min_timestep: int = None
    mixed_precision: str  = "no"
    mode_scale: float = 1.29
    network_alpha: float = 1
    network_args: str  = None
    network_dim: int = None
    network_dropout: float = None
    network_module: str  = None
    network_weights: str  = None
    no_metadata: bool = False
    optimizer_args: str  = None
    optimizer_type: str  = ""
    output_dir: str  = None
    output_name: str  = None
    persistent_data_loader_workers: bool = False
    resume: str  = None
    resume_from_huggingface: bool = False
    sage_attn: bool = False
    sample_at_first: bool = False
    sample_every_n_epochs: int = None
    sample_every_n_steps: int = None
    sample_prompts: str  = None
    save_every_n_epochs: int = None
    save_every_n_steps: int = None
    save_last_n_epochs: int = None
    save_last_n_epochs_state: int = None
    save_last_n_steps: int = None
    save_last_n_steps_state: int = None
    save_state: bool = False
    save_state_on_train_end: bool = False
    save_state_to_huggingface: bool = False
    scale_weight_norms: float = None
    sdpa: bool = False
    seed: int = None
    show_timesteps: str  = None
    sigmoid_scale: float = 1.0
    split_attn: bool = False
    t5: str  = None
    task: str  = "t2v-14B"
    timestep_sampling: str = "sigma"
    training_comment: str  = None
    vae: str  = None
    vae_cache_cpu: bool = False
    vae_dtype: str  = None
    wandb_api_key: str  = None # Don't display in frontend, no need send from frontend
    wandb_run_name: str  = None # Don't display in frontend, no need send from frontend
    weighting_scheme: str  = "none"
    xformers: bool = False